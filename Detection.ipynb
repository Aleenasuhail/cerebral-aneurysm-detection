{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkpMYLhq-ny_",
        "outputId": "a3752f75-1f18-4b3f-a57e-aa7b57a2b970"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLmuJbX7NHY3"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMuqOPlm-rEo"
      },
      "source": [
        "!pip install albumentations==0.4.6\n",
        "import albumentations \n",
        "pip install patichify \n",
        "pip install SimpleITK"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl8etiWH-2Th"
      },
      "source": [
        "\n",
        "import sys\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from datetime import datetime \n",
        "\n",
        "from PIL import Image\n",
        "from keras import backend, optimizers\n",
        "\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import models, layers, regularizers\n",
        "from tensorflow.keras import backend as K\n",
        "#from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
        "from matplotlib import pyplot as plt\n",
        "import os, glob\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
        "import imageio\n",
        "from albumentations import HorizontalFlip, VerticalFlip,CenterCrop, Rotate,RandomContrast,Transpose\n",
        "from tqdm import tqdm\n",
        "import imageio\n",
        "import numpy as np\n",
        "from skimage.io import imread_collection\n",
        "\n",
        "#import SimpleITK as sitk"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFOd2Hb0AWvk"
      },
      "source": [
        "MIP for image and label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JHaIAnLAWMi"
      },
      "source": [
        "dir = 'content/drive/MyDrive/ADAM-Filtered-Data/'\n",
        "output='/content/drive/MyDrive/Slices'\n",
        "for index,file_name in enumerate(sorted(os.listdir('/content/drive/MyDrive/ADAM-Filtered-Data'))):\n",
        "  if file_name.endswith('TOF.nii.gz'):\n",
        "    image=sitk.ReadImage('/content/drive/MyDrive/ADAM-Filtered-Data/' + file_name)\n",
        "    mip_axial = sitk.MaximumProjection(image, projectionDimension=2)[:,:,0]\n",
        "    #sitk.WriteImage(sitk.Cast(sitk.RescaleIntensity(mip_axial),sitk.sitkUInt8),output, f'axialMIP{index}.png')\n",
        "    sitk.WriteImage(sitk.Cast(sitk.RescaleIntensity(mip_axial),sitk.sitkUInt8),os.path.join(output, f'axialMIPimage{index}.png') )\n",
        "\n",
        "dir = 'content/drive/MyDrive/ADAM-Filtered-Data/'\n",
        "output2='/content/drive/MyDrive/Mask'\n",
        "for index,file_name in enumerate(sorted(os.listdir('/content/drive/MyDrive/ADAM-Filtered-Data'))):\n",
        "  if file_name.endswith('aneurysms.nii.gz'):\n",
        "    image=sitk.ReadImage('/content/drive/MyDrive/ADAM-Filtered-Data/' + file_name)\n",
        "    mip_axial = sitk.MaximumProjection(image, projectionDimension=2)[:,:,0]\n",
        "    #sitk.WriteImage(sitk.Cast(sitk.RescaleIntensity(mip_axial),sitk.sitkUInt8),output, f'axialMIP{index}.png')\n",
        "    sitk.WriteImage(sitk.Cast(sitk.RescaleIntensity(mip_axial),sitk.sitkUInt8),os.path.join(output2, f'axialMIPMask{index}.png') )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOLWZM7u-8aw"
      },
      "source": [
        "resizing test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edkf0U-H-8Ak"
      },
      "source": [
        "train_x=sorted(os.listdir('/content/drive/MyDrive/realpatch/test/images/'))\n",
        "train_y=sorted(os.listdir('/content/drive/MyDrive/realpatch/test/masks/'))\n",
        "\n",
        "\n",
        "#set size\n",
        "size=(256,256)\n",
        "  \n",
        "for x,y in tqdm(zip(train_x, train_y)):\n",
        "    #avoid overwritting \n",
        "    name=x.split('.')[0]\n",
        "    #read image \n",
        "    x=cv2.imread('/content/drive/MyDrive/realpatch/test/images/'+x)\n",
        "    y=cv2.imread('/content/drive/MyDrive/realpatch/test/masks/'+y)\n",
        "#reszie image and mask\n",
        "    x=cv2.resize(x,size)\n",
        "    y=cv2.resize(y,size)\n",
        "\n",
        "    tmp_image_name=f'{name}.png'\n",
        "      \n",
        "    tmp_mask_name=f'{name}.png'\n",
        "#save images masks\n",
        "    cv2.imwrite('/content/drive/MyDrive/realpatch/test/newtest/images/'+tmp_image_name, x)\n",
        "    cv2.imwrite(\"/content/drive/MyDrive/realpatch/test/newtest/masks/\"+tmp_mask_name, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCO1Da84_Acv"
      },
      "source": [
        "data augmentation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQtmXNux8BVV"
      },
      "source": [
        "#create directory\n",
        "def create_dir(path):\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "#load iamges and masks\n",
        "def load_data(path):\n",
        "  train_x=sorted(os.listdir(path+'/images/'))\n",
        "  train_y=sorted(os.listdir(path+\"/masks/\"))\n",
        "\n",
        "  \n",
        "\n",
        "  return (train_x, train_y)\n",
        "\n",
        "def augument_data(images, masks, save_path, augument=True):\n",
        "  size=(256,256)\n",
        "  \n",
        "  for x,y in tqdm((zip(images,masks)), total=len(images)):\n",
        "    \n",
        "    name=x.split('.')[0]\n",
        "    x=cv2.imread(data_path+'/images/'+x)\n",
        "    y=cv2.imread(data_path+\"/masks/\"+y)\n",
        "\n",
        "    if augument == True:\n",
        "      \n",
        "      aug=HorizontalFlip(p=1.0)\n",
        "      augumented=aug(image=x, mask=y)\n",
        "      x1=augumented['image']\n",
        "      y1=augumented['mask']\n",
        "\n",
        "      aug=VerticalFlip(p=1.0)\n",
        "      augumented=aug(image=x, mask=y)\n",
        "      x2=augumented['image']\n",
        "      y2=augumented['mask']\n",
        "\n",
        "     \n",
        "      aug=RandomContrast(limit=0.2, always_apply=True, p=0.5)\n",
        "      augumented=aug(image=x, mask=y)\n",
        "      x3=augumented['image']\n",
        "      y3=augumented['mask']\n",
        "\n",
        "      aug=Rotate(limit=20, p=1.0)\n",
        "      augumented=aug(image=x, mask=y)\n",
        "      x4=augumented['image']\n",
        "      y4=augumented['mask']\n",
        "\n",
        "      aug=Transpose(p=1)\n",
        "      augumented=aug(image=x, mask=y)\n",
        "      x5=augumented['image']\n",
        "      y5=augumented['mask']\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "      aug=Rotate(limit=35, p=1.0)\n",
        "      augumented=aug(image=x, mask=y)\n",
        "      x6=augumented['image']\n",
        "      y6=augumented['mask']\n",
        "\n",
        "     \n",
        "\n",
        "      aug=Rotate(limit=75, p=1.0)\n",
        "      augumented=aug(image=x, mask=y)\n",
        "      x7=augumented['image']\n",
        "      y7=augumented['mask']\n",
        "\n",
        "      aug=Rotate(limit=55, p=1.0)\n",
        "      augumented=aug(image=x, mask=y)\n",
        "      x8=augumented['image']\n",
        "      y8=augumented['mask']\n",
        "\n",
        "      aug=Rotate(limit=95, p=1.0)\n",
        "      augumented=aug(image=x, mask=y)\n",
        "      x9=augumented['image']\n",
        "      y9=augumented['mask']\n",
        "    \n",
        "\n",
        " \n",
        "      X=[x,x1,x2,x3,x4,x5,x6,x7,x8,x9]\n",
        "      Y=[y,y1,y2,y3,y4,y5,y6,y7,y8,y9]\n",
        "      \n",
        "      \n",
        "    else:\n",
        "      X=[x]\n",
        "      Y=[y]\n",
        "    \n",
        "    for index, (i, m) in enumerate(zip(X,Y)):\n",
        "      i=cv2.resize(i,size)\n",
        "      m=cv2.resize(m,size) \n",
        "\n",
        "      tmp_image_name=f'{name}_{index}.png'\n",
        "      tmp_mask_name=f'{name}_{index}.png'\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "      cv2.imwrite(data_path+'new/images/'+tmp_image_name, i)\n",
        "      cv2.imwrite(data_path+'new/masks/'+tmp_mask_name, m)\n",
        "\n",
        "if __name__=='__main__':\n",
        "  np.random.seed(42)\n",
        "  #Specify the path\n",
        "  data_path = \"/content/drive/MyDrive/realpatch/train/new3/\"\n",
        "  (train_x, train_y)=load_data(data_path)\n",
        "\n",
        "  #Directories for augumented data\n",
        "  create_dir(data_path+\"new/images/\")\n",
        "  create_dir(data_path+\"new/masks/\")\n",
        "  \n",
        " #create save new images\n",
        "  augument_data(train_x, train_y,'/content/drive/MyDrive/realpatch/train/new3/new/', augument=True)\n",
        "\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IXdlS00vvCy"
      },
      "source": [
        "extract patches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-qIHkNGAfNw"
      },
      "source": [
        "large_image_stack=sorted(os.listdir('/content/drive/MyDrive/MRA/traincrop/images/'))\n",
        "large_mask_stack=sorted(os.listdir('/content/drive/MyDrive/MRA/traincrop/masks/'))\n",
        "for x in large_image_stack:\n",
        "  name=x.split('.')[0]\n",
        "  #read images\n",
        "  img = cv2.imread(\"/content/drive/MyDrive/MRA/traincrop/masks/\"+x)\n",
        "  #use patchify library to extraxt patches \n",
        "  #patch size 64 overlap in middle (32)\n",
        "  patches_img = patchify(img, (64,64,3), step=32)\n",
        "  tmp_image_name=f'{name}'\n",
        "  for i in range(patches_img.shape[0]):\n",
        "      for j in range(patches_img.shape[1]):\n",
        "          single_patch_img = patches_img[i, j, 0, :, :, :]\n",
        "          if not cv2.imwrite('/content/drive/MyDrive/testpatchi/masks/' + tmp_image_name+'_'+ str(i)+str(j)+'.png', single_patch_img):\n",
        "            raise Exception(\"Could not write the image\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvR9EiHz_gis"
      },
      "source": [
        "dice loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22rjAeVq_f5O"
      },
      "source": [
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2.0 * intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1.0)\n",
        "\n",
        "\n",
        "def jacard_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
        "\n",
        "\n",
        "def jacard_coef_loss(y_true, y_pred):\n",
        "    return -jacard_coef(y_true, y_pred)\n",
        "\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return 1-dice_coef(y_true, y_pred)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO6rt-MF_maQ"
      },
      "source": [
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p19u-Ne_iBa"
      },
      "source": [
        "\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "#convolution block\n",
        "def cblock(input, output):\n",
        "    x = Conv2D(output, 3, padding=\"same\")(input) #kernel size=3, zero padding \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "  \n",
        "    x = Conv2D(output, 3, padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "#encoder block\n",
        "def encoder_block(input, output):\n",
        "    x = cblock(input, output) \n",
        "        \n",
        "    p = MaxPool2D((2, 2))(x)    #2 x2 pooling layer \n",
        "    return x, p #output of conv block and maxpooling layer\n",
        "\n",
        "#decoder block\n",
        "def decoder_block(input, skip_output, output):\n",
        "    x = Conv2DTranspose(output, (2, 2), strides=2, padding=\"same\",)(input)\n",
        " \n",
        "    x = Concatenate()([x, skip_output])\n",
        "    x = cblock(x, output)\n",
        "    return x\n",
        "\n",
        "def unet(input_shape): #encoder-bridge-decoder-final layer\n",
        "    inputs = Input(input_shape)\n",
        "#4 encoder layer\n",
        "    s1, p1 = encoder_block(inputs,32)\n",
        "    s2, p2 = encoder_block(p1, 64)\n",
        "    s3, p3 = encoder_block(p2, 128)\n",
        "    s4, p4 = encoder_block(p3, 256)\n",
        "#bridge\n",
        "    b1 = cblock(p4, 512)\n",
        "#4 decoder layers\n",
        "    d1 = decoder_block(b1, s4, 256)\n",
        "    d2 = decoder_block(d1, s3, 128)\n",
        "    d3 = decoder_block(d2, s2, 64)\n",
        "    d4 = decoder_block(d3, s1, 32)\n",
        "#classification layer\n",
        "    final = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n",
        "\n",
        "    model = Model(inputs, final, name=\"U-Net\")\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_shape = (256, 256, 3) #no batchsize specified\n",
        "    model = unet(input_shape)\n",
        "   # model.summary()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt7PpQ4U_oxn"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "image_directory = '/content/drive/MyDrive/realpatch/train/new/images/'\n",
        "mask_directory = '/content/drive/MyDrive/realpatch/train/new/masks/'\n",
        "\n",
        "\n",
        "\n",
        "image_dataset = []  #for images \n",
        "mask_dataset = []  #for masks\n",
        "#load resize\n",
        "images = sorted(os.listdir(image_directory))\n",
        "for i, image_name in enumerate(images):    \n",
        "  \n",
        "    image = cv2.imread(image_directory+image_name, 1)\n",
        "    image = Image.fromarray(image)\n",
        "   \n",
        "    image_dataset.append(np.array(image))\n",
        "\n",
        "\n",
        "\n",
        "masks = sorted(os.listdir(mask_directory))\n",
        "for i, image_name in enumerate(masks):\n",
        "  image = cv2.imread(mask_directory+image_name, 0)\n",
        "  image = Image.fromarray(image)\n",
        "  mask_dataset.append(np.array(image))\n",
        "\n",
        "\n",
        "#Normalise images\n",
        "image_dataset = np.array(image_dataset)/255.\n",
        "#rescale label\n",
        "mask_dataset = np.expand_dims((np.array(mask_dataset)),3) /255.\n",
        "\n",
        "\n",
        "#splitdata into training val\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(image_dataset, mask_dataset, test_size = 0.05, random_state = 0)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvj5CUCUJ8Jt",
        "outputId": "4c63021f-6777-4f01-bb4b-ca4f367aeefb"
      },
      "source": [
        "len(X_test)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2PVczpE_vlh"
      },
      "source": [
        "IMG_HEIGHT = X_train.shape[1]\n",
        "IMG_WIDTH  = X_train.shape[2]\n",
        "IMG_CHANNELS = X_train.shape[3]\n",
        "num_labels = 1  #Binary\n",
        "input_shape = (IMG_HEIGHT,IMG_WIDTH,IMG_CHANNELS)\n",
        "#Hyperparameters\n",
        "batch_size = 16\n",
        "num_epochs=200\n",
        "l_r=0.005\n",
        "#load model \n",
        "unet_model = unet(input_shape)\n",
        "unet_model.compile(optimizer=Adam(lr = l_r), loss=dice_coef_loss, \n",
        "              metrics=['accuracy', jacard_coef,dice_coef])\n",
        "unet_data = unet_model.fit(X_train, y_train, \n",
        "                    verbose=1,\n",
        "                    batch_size = batch_size,\n",
        "                    validation_data=(X_test, y_test ), \n",
        "                    shuffle=False,\n",
        "                    epochs=num_epochs)\n",
        "#save model\n",
        "unet_model.save('Unet.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO1Pwib6_1nI"
      },
      "source": [
        "\n",
        "#write loss jacard coeficient to pandas dataframe\n",
        "unet_history_df = pd.DataFrame(unet_data.history) \n",
        "\n",
        "with open('unet_history_df.csv', mode='w') as f:\n",
        "    unet_history_df.to_csv(f) "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpjtoPuv_43L"
      },
      "source": [
        "\n",
        "history = unet_data\n",
        "\n",
        "#plot the training and validation loss and jaccard coef per epoch\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "acc = history.history['jacard_coef']\n",
        "\n",
        "val_acc = history.history['val_jacard_coef']\n",
        "\n",
        "\n",
        "plt.plot(epochs, acc, 'y', label='Training Jacard')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation Jacard')\n",
        "plt.title('Training and validation Jacard')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Jacard')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKjC6G-p_7sn"
      },
      "source": [
        "import random\n",
        "test_img_number = random.randint(0, X_test.shape[0]-1)\n",
        "test_img = X_test[test_img_number]\n",
        "ground_truth=y_test[test_img_number]\n",
        "\n",
        "test_img_input=np.expand_dims(test_img, 0)\n",
        "prediction = (model.predict(test_img_input)[0,:,:,0] > 0.5).astype(np.uint8)\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(231)\n",
        "\n",
        "plt.imshow(test_img, cmap='gray')\n",
        "plt.subplot(232)\n",
        "\n",
        "plt.imshow(ground_truth[:,:,0], cmap='gray')\n",
        "plt.subplot(233)\n",
        "\n",
        "\n",
        "plt.imshow(prediction, cmap='gray')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "from tensorflow.keras.metrics import MeanIoU\n",
        "n_classes = 2\n",
        "IOU_keras = MeanIoU(num_classes=n_classes)  \n",
        "IOU_keras.update_state(ground_truth[:,:,0], prediction)\n",
        "print(\"Mean IoU =\", IOU_keras.result().numpy())\n",
        "\n",
        "\n",
        "#Calculate IoU for all test images and average\n",
        " \n",
        "import pandas as pd\n",
        "\n",
        "IoU_values = []\n",
        "for img in range(0, X_test.shape[0]):\n",
        "    temp_img = X_test[img]\n",
        "    ground_truth=y_test[img]\n",
        "    temp_img_input=np.expand_dims(temp_img, 0)\n",
        "    prediction = (model.predict(temp_img_input)[0,:,:,0] > 0.5).astype(np.uint8)\n",
        "    \n",
        "    IoU = MeanIoU(num_classes=n_classes)\n",
        "    IoU.update_state(ground_truth[:,:,0], prediction)\n",
        "    IoU = IoU.result().numpy()\n",
        "    IoU_values.append(IoU)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "df = pd.DataFrame(IoU_values, columns=[\"IoU\"])\n",
        "df = df[df.IoU != 1.0]    \n",
        "mean_IoU = df.mean().values\n",
        "print(\"Mean IoU is: \", mean_IoU)    \n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_SpheJtNPi8"
      },
      "source": [
        "Some reference for the code has been used from the following github link:\n",
        "<bnsreenu> <https://github.com/bnsreenu/python_for_microscopists/blob/master/224_225_226_mito_segm_using_various_unet_models.py> \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Opp8VthTjgar"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}